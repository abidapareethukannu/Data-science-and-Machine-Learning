{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNZ4vIdFwcrldghMoFho99a"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["***NATURAL LANGUAGE PROCESSING ***\n","\n"],"metadata":{"id":"zdo82tQoOURN"}},{"cell_type":"code","source":["import nltk\n","nltk.download('punkt')\n","nltk.download('punkt_tab')\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('averaged_perceptron_tagger_eng')\n","nltk.download('wordnet')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aIHqJ2lE-iAB","executionInfo":{"status":"ok","timestamp":1755149208715,"user_tz":-330,"elapsed":585,"user":{"displayName":"Devika Rajendran","userId":"14590572194786065352"}},"outputId":"43870e50-eb2c-49f8-bcaa-75fd37bf7373"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n","[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":18}]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6Ir0KioX0GEG","executionInfo":{"status":"ok","timestamp":1755149278446,"user_tz":-330,"elapsed":30441,"user":{"displayName":"Devika Rajendran","userId":"14590572194786065352"}},"outputId":"851c991e-aef7-4322-b376-733f9465bfa2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Enter the sentence: Data Science\n","Enter the value of n: 3\n","ngrams printing\n","tokens printing\n","['Data', 'Science']\n","['Data Science']\n","[('Data', 'NNS'), ('Science', 'NN')]\n","Data : data\n","Science : scienc\n"]}],"source":["from nltk import ngrams, word_tokenize, sent_tokenize, pos_tag\n","from nltk.stem import PorterStemmer\n","sentence = input(\"Enter the sentence: \")\n","n = int(input(\"Enter the value of n: \"))\n","n_grams = ngrams(sentence.split(), n)\n","print(\"ngrams printing\")\n","for grams in n_grams:\n"," print(grams)\n","print(\"tokens printing\")\n","print(word_tokenize(sentence))\n","print(sent_tokenize(sentence))\n","tokenized_text = word_tokenize(sentence)\n","tags = pos_tag(tokenized_text)\n","print(tags)\n","ps = PorterStemmer()\n","for w in tokenized_text:\n"," print(w, \":\", ps.stem(w))"]}]}